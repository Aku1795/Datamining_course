{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxAwwRBFxFCh"
   },
   "source": [
    "# Etude de cas CDiscount à l'aide de l'environnement pyspark\n",
    "\n",
    "Ce notebook reprend le cas CDIscount mais cette fois ci à l'aide de l'environnement pyspark.\n",
    "\n",
    "## 0. Importation des premiers modules.\n",
    "\n",
    "Nous nous baserons ici sur pyspark qui devrait déjà être installé et en particulier nous utiliserons les modules ci-dessous pour cette étude de cas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kwO76j-xFCj"
   },
   "outputs": [],
   "source": [
    "##Nettoyage des données\n",
    "import nltk\n",
    "import re\n",
    "##Liste\n",
    "from numpy import array\n",
    "##Temps\n",
    "import time\n",
    "\n",
    "from pyspark import SparkContext\n",
    "##Row and Vector\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "##Hachage et vectorisation\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "##Regression logistique\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "##Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "##Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "##Pour la création des DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPQFvK4pxFCn"
   },
   "source": [
    "### Spark context initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybpTDf9GxFCo"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "sc = SparkContext(\"local[*]\",\"CDiscount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rh4fSBinxFCt"
   },
   "source": [
    "## 1. Impoi\n",
    "\n",
    "Il s'agit ici d'importer vos données.\n",
    "Il faudra donc:\n",
    " + Créer la base distribuée à partir du fichier csv.\n",
    " + Créer le dataframe spark SQL correspondant.\n",
    " + Completer les valeurs manquantes par la chaîne de caractères nulle.\n",
    " + Afficher les 10 premières lignes du dataframe obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kVtoYNdxFCu"
   },
   "outputs": [],
   "source": [
    "# Création de la base distribuée avec SQL context\n",
    "# TO COMPLETE\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./Data/Categorie_reduit.csv'\n",
    "                   , sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ph-0ujJLxFCw"
   },
   "outputs": [],
   "source": [
    "# Vérification du dataframe obtenu : son schema \n",
    "df = df.selectExpr('_c0 as Categorie1'\n",
    "              ,'_c1 as Categorie2'\n",
    "              ,'_c2 as Categorie3'\n",
    "              ,'_c3 as Description'\n",
    "              ,'_c4 as Libelle'\n",
    "              ,'_c5 as Marque')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling na\n",
    "df = df.na.fill(\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTrOyCUYxFCy"
   },
   "source": [
    "Combien avez-vous de produits dans le fichier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOE36VkoxFC1"
   },
   "source": [
    "Le fichier a beaucoup de lignes. Comme avec python, dans un premier temps, nous allons travailler sur une sous-partie des données. Nous allons pour cela utiliser la fonction `randomSplit` de pyspark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClTDhnqExFC3"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "taux_donnees= .80 # TO COMPLETE\n",
    "datakeep,data_drop = df.randomSplit([taux_donnees, 1 - taux_donnees])  # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMPrpg8IxFDL"
   },
   "source": [
    "## 2. Séparation des données en un ensemble d'apprentissage et un ensemble de validation.\n",
    "\n",
    "Nous allons ici séparer l'ensemble des données disponibles en 2 sous-ensembles, un ensemble pour apprendre le modèle de prédiction, i.e. l'ensemble d'apprentissage et un ensemble de validation. On utilisera aussi la fonction randomSplit de pyspark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fE_2lGLxFDM"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "tauxsep= .80 # TO COMPLETE\n",
    "(trainDF, testDF) = datakeep.randomSplit([tauxsep, 1-tauxsep]) # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgp2RI5lxFDR"
   },
   "source": [
    "## 3. Nettoyage des données\n",
    "\n",
    "Nous devons traiter des données textuelles et il nous faudra donc construire une représentation numérique de ces données. Pour cela, il est d'abord nécessaire de nettoyer ces données. \n",
    "\n",
    "Dans cette étude de cas, nous représenterons les données à partir de la liste des mots les constituant et il faudra donc :\n",
    " + Construire un dictionnaire de mots. Ce dictionnaire de mot sera l'espace de représentation de vos données. Pour cela, il  vous faut : \n",
    "  + Découper le texte en mots \n",
    "   + Nettoyer le texte, le simplifier : suppression des ponctuations, des termes numériques, des caractéres mal codés, passage de tous les mots en minuscules.\n",
    "   + Supprimer les mots non porteurs de sens (ou stop words) à l'aide de la liste `lucene_stopwords.txt`.\n",
    "   + Lemmatiser ou Raciniser (transformer un mot en sa forme canonique) afin de reduire la taille du dictionnaire et donc l'espace de représentation des données.\n",
    "   \n",
    "Vous pourrez pour cela utiliser la bibiothèque MLIB. La documentation est [ici](https://spark.apache.org/docs/2.2.0/ml-features.html).\n",
    "\n",
    "Pour la racinisation, il faudra faire appel, par contre, à la bibliothèque nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvICj_oqxFDS"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# liste des mots à supprimer\n",
    "# TO COMPLETE\n",
    "start_time = time.time()\n",
    "\n",
    "stopwords = list(stopwords.words('French'))\n",
    "\n",
    "# Tokenisation : remplacer un texte par la liste de ses mots\n",
    "tokenizer = RegexTokenizer(inputCol=\"Description\", outputCol=\"Descrip_tokenized\")\n",
    "trainDF = tokenizer.transform(trainDF)\n",
    "testDF = tokenizer.transform(testDF)\n",
    "\n",
    "\n",
    "# TO COMPLETE\n",
    "# Filtrage Stop Words\n",
    "remover = StopWordsRemover(inputCol = 'Descrip_tokenized', outputCol = 'Descrip_clen')\n",
    "trainDF = remover.transform(trainDF)\n",
    "testDF = remover.transform(testDF)\n",
    "\n",
    "# TO COMPLETE\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import  ArrayType, StringType\n",
    "\n",
    "def stemmer(words, language = 'french'):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "udf_stemmer = udf(stemmer, ArrayType(StringType()))\n",
    "\n",
    "trainDF = trainDF.withColumn('Description_finale', udf_stemmer('Descrip_clen'))\n",
    "testDF = testDF.withColumn('Description_finale', udf_stemmer('Descrip_clen'))\n",
    "\n",
    "# TO COMPLETE\n",
    "trainDF = trainDF.drop('Descrip_tokenized', 'Descrip_clen')\n",
    "testDF = trainDF.drop('Descrip_tokenized', 'Descrip_clen')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"elapsed_time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkgQIqGJxFDW"
   },
   "source": [
    "## 4.  Représentation des données.\n",
    "\n",
    "Pour représenter nos données (i.e. la description textuelle des produits), plusieurs principes seront utilisés et comparés :\n",
    "\n",
    "\n",
    " + L'approche de représentation d'un document textuel par un sac de mots et une pondération [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) vue dans les premiers cours disponible dans MLib : doucmentation [ici](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf) et qui intégre l'approche de hachage comme expliquée [ici](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.HashingTF) \n",
    " + Une représentation de type word2vec, toujours avec Mlib: [ici](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#word2vec)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2Bo5FJlxFDX"
   },
   "source": [
    "#### TF-IDF et hachage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4Sq-Z-7cxFDY"
   },
   "outputs": [],
   "source": [
    "# representation TF-IDF\n",
    "# TO COMPLETE\n",
    "hashingTF = HashingTF(inputCol=\"Description_finale\", outputCol=\"rawFeatures\")\n",
    "trainDF = hashingTF.transform(trainDF)\n",
    "testDF = hashingTF.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "idf = IDF(inputCol = 'rawFeatures', outputCol = 'features')\n",
    "idf_model = idf.fit(trainDF)\n",
    "trainDF_IDF = idf_model.transform(trainDF)\n",
    "testDF_IDF = idf_model.transform(testDF)\n",
    "end_time = time.time()\n",
    "\n",
    "print('elapsed time: ', end_time - start_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_IDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eT4picf5xFDa"
   },
   "outputs": [],
   "source": [
    "# Application à votre jeu de données et mesure du temps\n",
    "# TO COMPLETE\n",
    "trainDF_IDF = trainDF_IDF.selectExpr(\n",
    "    'Categorie1',\n",
    " 'Categorie2',\n",
    " 'Categorie3',\n",
    " 'features as features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Categorie1\", outputCol=\"label\")\n",
    "indexer = indexer.fit(trainDF_IDF)\n",
    "trainDF_IDF = indexer.transform(trainDF_IDF)\n",
    "testDF_IDF = indexer.transform(testDF_IDF)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Etude_De_Cas_CDiscount_Spark_Final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
