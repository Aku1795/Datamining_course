{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTvZWqhWyAbt"
   },
   "source": [
    "# Etude de cas CDiscount à l'aide de l'environnement python : pandas et scikit-learn\n",
    "\n",
    "Ce notebook vous propose une trame de travail pour le cas CDiscount à l'aide de pandas et de scikit-learn. Il reprend les différentes étapes de l'énoncé.\n",
    "\n",
    "\n",
    "## 0. Importation des premiers modules.\n",
    "\n",
    "Nous nous baserons ici sur la bibliothèque `pandas` et `scikit-learn`. Votre premier travail consiste donc à les installer si ce n'est pas le cas avec la commande `pip` ou `conda` selon comment vous avez installé python sur votre ordinateur. Nous aurons aussi besoin de la bibliothèque `nltk` ([`spaCy`](https://spacy.io/) plus récente mais de plus en plus repandue pourrait aussi être utilisée pour cette partie) pour travailler sur les données qui sont des données textuelles ainsi que du module [`time`](https://docs.python.org/3.7/library/time.html) de python pour permettre de mesurer les performances en terme de temps de calcul des différentes étapes.\n",
    "\n",
    "Vous pouvez vous référer à la documentation :\n",
    " + Pour pandas: [ici](https://pandas.pydata.org/pandas-docs/stable/install.html)\n",
    " + Pour scikit-learn : [ici](https://scikit-learn.org/stable/)\n",
    " + Pour nltk : [ici](https://www.nltk.org/install.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZxNnJOtyAbu"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8 \n",
    "# TO DO : Importer les modules principaux nécéssaires à cette étude de cas\n",
    "\n",
    "\n",
    "# PANDAS : e.g import pandas as pd\n",
    "import pandas as pd\n",
    "# NUMPY \n",
    "import numpy as np\n",
    "\n",
    "# CSV pour le chargement des données\n",
    "\n",
    "# TIME pour la mesure des performances\n",
    "import time\n",
    "# nltk pour le traitement des données textuelles\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbxjsP6dyAby"
   },
   "source": [
    "## 1. Importation des données\n",
    "\n",
    "Il s'agit ici de préparer votre répertoire de travail et de définir les différentes variables globales de votre projet.\n",
    "Il faudra donc:\n",
    " + Télécharger les fichiers `Categorie_reduit.csv` et `lucene_stopwords.txt` disponibles [ici](https://drive.google.com/drive/folders/1lssbStcS1acUO4AFlWCKOwel4dLTSylo?usp=sharing).\n",
    " + Les placer dns le répertoire `.\\Data` dans votre répertoire de travail afin de pouvoir y accèder facilement.\n",
    " + Définir la variable `HEADER_CDISCOUNT_DATA =['Categorie1','Categorie2','Categorie3','Description','Libelle','Marque']` pour pouvoir accèder plus facilement aux données.\n",
    " + Créer un dataframe pandas à partir du fichier csv et dont les champs sont ceux définis dans la variable `HEADER_CDISCOUNT_DATA`.\n",
    " + Si vous avez des problèmes d'encodage, fréquents avec les caractères accentués en Français, vous pouvez prendre le temps de lire ce [tutoriel](http://sametmax.com/lencoding-en-python-une-bonne-fois-pour-toute/) ou \n",
    " + Completer les données manquantes par une chaine de caractères vide.\n",
    " + Afficher les 10 premières lignes du dataframe obtenu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Urov-ybOyAby"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "HEADER_CDISCOUNT_DATA =['Categorie1','Categorie2','Categorie3','Description','Libelle','Marque']\n",
    "# TO COMPLETE\n",
    "df = pd.read_csv('./Data/Categorie_reduit.csv'\n",
    "                 , sep = ';'\n",
    "                 , names = HEADER_CDISCOUNT_DATA)\n",
    "df.fillna(np.NaN, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000015308</td>\n",
       "      <td>1000015309</td>\n",
       "      <td>La chamade de Françoise Sagan  - La chamade de...</td>\n",
       "      <td>La chamade de Françoise Sagan</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000005258</td>\n",
       "      <td>1000005707</td>\n",
       "      <td>1000005986</td>\n",
       "      <td>Protections diverses - Derbi - 50 - Senda - Pr...</td>\n",
       "      <td>Protections diverses - Derbi - 50 - Senda</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000008694</td>\n",
       "      <td>1000008920</td>\n",
       "      <td>1000008939</td>\n",
       "      <td>AUDI TT 1/40 GRISE RC - AUDI TT 1/40 GRISE RC…...</td>\n",
       "      <td>AUDI TT 1-40 GRISE RC</td>\n",
       "      <td>JAMARA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193</td>\n",
       "      <td>1449</td>\n",
       "      <td>194</td>\n",
       "      <td>La Sangre Llama - Personnel includes: Luis Var...</td>\n",
       "      <td>La Sangre Llama</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque souple Bleue pour LG G3 motif Drapeau fi...</td>\n",
       "      <td>Coque souple Bleue pour LG G3 motif Drapeau fin…</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000010096</td>\n",
       "      <td>1000010097</td>\n",
       "      <td>1000010110</td>\n",
       "      <td>Bracelet manchette turquoise pastel de grandes...</td>\n",
       "      <td>Bracelet manchette turquoise pastel de grandes...</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000010220</td>\n",
       "      <td>1000010327</td>\n",
       "      <td>1000010334</td>\n",
       "      <td>Casquette ETNIES Catch 22 Artbl - Casquette ET...</td>\n",
       "      <td>Casquette ETNIES Catch 22 Artbl</td>\n",
       "      <td>Etnies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000013441</td>\n",
       "      <td>1000013731</td>\n",
       "      <td>1000013776</td>\n",
       "      <td>Little John pour Flûte et Piano - Auteur: Mich...</td>\n",
       "      <td>Little John pour Flûte et Piano</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000002677</td>\n",
       "      <td>1000002684</td>\n",
       "      <td>1000004580</td>\n",
       "      <td>Lien nylon Ace - Dimensions 3,6 x 180 mm - Par...</td>\n",
       "      <td>Lien nylon Ace - Dimensions 3,6 x 180 mm - Par...</td>\n",
       "      <td>ACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000012160</td>\n",
       "      <td>1000012240</td>\n",
       "      <td>1000012243</td>\n",
       "      <td>ABATOUT - Laque insecticide spécialement conçu...</td>\n",
       "      <td>ABATOUT</td>\n",
       "      <td>ABATOUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Categorie1  Categorie2  Categorie3  \\\n",
       "0  1000014006  1000015308  1000015309   \n",
       "1  1000005258  1000005707  1000005986   \n",
       "2  1000008694  1000008920  1000008939   \n",
       "3         193        1449         194   \n",
       "4  1000010560  1000010623  1000010653   \n",
       "5  1000010096  1000010097  1000010110   \n",
       "6  1000010220  1000010327  1000010334   \n",
       "7  1000013441  1000013731  1000013776   \n",
       "8  1000002677  1000002684  1000004580   \n",
       "9  1000012160  1000012240  1000012243   \n",
       "\n",
       "                                         Description  \\\n",
       "0  La chamade de Françoise Sagan  - La chamade de...   \n",
       "1  Protections diverses - Derbi - 50 - Senda - Pr...   \n",
       "2  AUDI TT 1/40 GRISE RC - AUDI TT 1/40 GRISE RC…...   \n",
       "3  La Sangre Llama - Personnel includes: Luis Var...   \n",
       "4  Coque souple Bleue pour LG G3 motif Drapeau fi...   \n",
       "5  Bracelet manchette turquoise pastel de grandes...   \n",
       "6  Casquette ETNIES Catch 22 Artbl - Casquette ET...   \n",
       "7  Little John pour Flûte et Piano - Auteur: Mich...   \n",
       "8  Lien nylon Ace - Dimensions 3,6 x 180 mm - Par...   \n",
       "9  ABATOUT - Laque insecticide spécialement conçu...   \n",
       "\n",
       "                                             Libelle   Marque  \n",
       "0                     La chamade de Françoise Sagan    AUCUNE  \n",
       "1          Protections diverses - Derbi - 50 - Senda   AUCUNE  \n",
       "2                              AUDI TT 1-40 GRISE RC   JAMARA  \n",
       "3                                    La Sangre Llama   AUCUNE  \n",
       "4   Coque souple Bleue pour LG G3 motif Drapeau fin…  MUZZANO  \n",
       "5  Bracelet manchette turquoise pastel de grandes...   AUCUNE  \n",
       "6                    Casquette ETNIES Catch 22 Artbl   Etnies  \n",
       "7                    Little John pour Flûte et Piano   AUCUNE  \n",
       "8  Lien nylon Ace - Dimensions 3,6 x 180 mm - Par...      ACE  \n",
       "9                                            ABATOUT  ABATOUT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agHbqIlJyAb1"
   },
   "source": [
    "Combien avez-vous de produits dans le fichier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quGjuZv8yAb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 fichiers\n"
     ]
    }
   ],
   "source": [
    "# TO DO : afficher les informations sur le nombre d'éléments dans le fichier.\n",
    "print(df.shape[0], \"fichiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MsBnyB3-yAb4"
   },
   "source": [
    "Le fichier a beaucoup de lignes. Dans un premier temps, nous allons travailler sur une sous-partie des données. Definissez une variable `nb_lines` permettra de tester sur un nombre restreint de lignes des données source. Vous pouvez par exemple vous limitez à 20000 lignes au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfEbOaGyyAb5"
   },
   "outputs": [],
   "source": [
    "nb_lines = 20000\n",
    "# TO COMPLETE\n",
    "df_limited = df.iloc[:20000,:]\n",
    "# TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUucoBkUyAb8"
   },
   "source": [
    "## 2. Séparation des données en un ensemble d'apprentissage et un ensemble de validation.\n",
    "\n",
    "Nous allons ici séparer l'ensemble des données disponibles en 2 sous-ensembles, un ensemble pour apprendre le modèle de prédiction, i.e. l'ensemble d'apprentissage et un ensemble de validation. Pour cela nous allons utiliser la bibliothèque `scikit-learn`, qu'il faudra donc importer et sa fonction [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) du module [`model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). \n",
    "\n",
    "Il vous faudra ici pouvoir mesurer le temps mis pour réaliser cette opération avec le module `time` de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6c7GmkqyAb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 0.022844791412353516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcthanvancon/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 1.055466890335083\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Importer la fonction train_test_split de scikit_learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TO COMPLETE : fonction split_dataset qui sépare un jeu de donnés initial en 2 sous ensembles en fonction de la valeur donnée dans taux_sep\n",
    "\n",
    "def split_dataset(initial_data, taux_sep):\n",
    "    # TO COMPLETE\n",
    "    start_time = time.time()\n",
    "    X_train, X_test = train_test_split(initial_data, train_size= taux_sep )\n",
    "    end_time = time.time()\n",
    "    print(\"elapsed time\", end_time-start_time)\n",
    "    return X_train, X_test\n",
    "\n",
    "    \n",
    "# TO COMPLETE = application sur le jeu de données    \n",
    "taux_sep= 1-0.20\n",
    "df_train_lm, df_test_lm = split_dataset(df_limited, taux_sep)\n",
    "df_train, df_test = split_dataset(df, taux_sep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29dR5mEmyAb_"
   },
   "source": [
    "Afficher les 10 premieres lignes des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EMSIY1OyAcA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>985267</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Samsung Galaxy S3 Mini I8190 coque dure case n...</td>\n",
       "      <td>Samsung Galaxy S3 Mini I8190 coque dure case n...</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964344</th>\n",
       "      <td>1000003924</td>\n",
       "      <td>1000011472</td>\n",
       "      <td>1000011473</td>\n",
       "      <td>MICROSCREEN MSC32026 ÉCRAN POUR ORDINATEUR POR...</td>\n",
       "      <td>MICROSCREEN MSC32026 ÉCRAN POUR ORDINATEUR PORT…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586300</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque  LG Optimus L7 Smartphone angleterre rig...</td>\n",
       "      <td>Coque  LG Optimus L7 Smartphone angleterre rigi…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536214</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque souple Violette pour LG G FLEX motif I L...</td>\n",
       "      <td>Coque souple Violette pour LG G FLEX motif I LO…</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650788</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque Kindle Fire HD 7 pouces (Version 2013) –...</td>\n",
       "      <td>Coque Kindle Fire HD 7 pouces (Version 2013) – …</td>\n",
       "      <td>SUTEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922020</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010667</td>\n",
       "      <td>Housse Pull Tab Taille S Apple iPod Touch 5Exc...</td>\n",
       "      <td>Housse Pull Tab Taille S Apple iPod Touch 5</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737860</th>\n",
       "      <td>1000010096</td>\n",
       "      <td>1000010097</td>\n",
       "      <td>1000010108</td>\n",
       "      <td>Or blanc 375/1000 - Poids moyen de l'or : 0,83...</td>\n",
       "      <td>OR ECLAT Boucles d'Oreilles Or Blanc 375° Femme</td>\n",
       "      <td>OR ECLAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64308</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010647</td>\n",
       "      <td>CHARGEUR SECTEUR CLASSIC 2USB 1AMPÈRE POUR SMA...</td>\n",
       "      <td>CHARGEUR SECTEUR CLASSIC 2USB 1AMPÈRE POUR SMAR…</td>\n",
       "      <td>ENERGIZER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952019</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000015308</td>\n",
       "      <td>1000015309</td>\n",
       "      <td>De Michel Ogrizek aux éditions APOGEE</td>\n",
       "      <td>ENVIRONNEMENT ET COMMUNICATION</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754339</th>\n",
       "      <td>1000002514</td>\n",
       "      <td>1000002590</td>\n",
       "      <td>1000002604</td>\n",
       "      <td>Support 130cm en bois et métal pour bois de ch...</td>\n",
       "      <td>Support 130cm en bois et métal pour bois de ch...</td>\n",
       "      <td>MASSIVUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Categorie1  Categorie2  Categorie3  \\\n",
       "985267  1000010560  1000010623  1000010653   \n",
       "964344  1000003924  1000011472  1000011473   \n",
       "586300  1000010560  1000010623  1000010653   \n",
       "536214  1000010560  1000010623  1000010653   \n",
       "650788  1000010560  1000010623  1000010653   \n",
       "922020  1000010560  1000010623  1000010667   \n",
       "737860  1000010096  1000010097  1000010108   \n",
       "64308   1000010560  1000010623  1000010647   \n",
       "952019  1000014006  1000015308  1000015309   \n",
       "754339  1000002514  1000002590  1000002604   \n",
       "\n",
       "                                              Description  \\\n",
       "985267  Samsung Galaxy S3 Mini I8190 coque dure case n...   \n",
       "964344  MICROSCREEN MSC32026 ÉCRAN POUR ORDINATEUR POR...   \n",
       "586300  Coque  LG Optimus L7 Smartphone angleterre rig...   \n",
       "536214  Coque souple Violette pour LG G FLEX motif I L...   \n",
       "650788  Coque Kindle Fire HD 7 pouces (Version 2013) –...   \n",
       "922020  Housse Pull Tab Taille S Apple iPod Touch 5Exc...   \n",
       "737860  Or blanc 375/1000 - Poids moyen de l'or : 0,83...   \n",
       "64308   CHARGEUR SECTEUR CLASSIC 2USB 1AMPÈRE POUR SMA...   \n",
       "952019              De Michel Ogrizek aux éditions APOGEE   \n",
       "754339  Support 130cm en bois et métal pour bois de ch...   \n",
       "\n",
       "                                                  Libelle     Marque  \n",
       "985267  Samsung Galaxy S3 Mini I8190 coque dure case n...     AUCUNE  \n",
       "964344   MICROSCREEN MSC32026 ÉCRAN POUR ORDINATEUR PORT…     AUCUNE  \n",
       "586300   Coque  LG Optimus L7 Smartphone angleterre rigi…     AUCUNE  \n",
       "536214   Coque souple Violette pour LG G FLEX motif I LO…    MUZZANO  \n",
       "650788   Coque Kindle Fire HD 7 pouces (Version 2013) – …      SUTEO  \n",
       "922020        Housse Pull Tab Taille S Apple iPod Touch 5     AUCUNE  \n",
       "737860    OR ECLAT Boucles d'Oreilles Or Blanc 375° Femme   OR ECLAT  \n",
       "64308    CHARGEUR SECTEUR CLASSIC 2USB 1AMPÈRE POUR SMAR…  ENERGIZER  \n",
       "952019                     ENVIRONNEMENT ET COMMUNICATION        NaN  \n",
       "754339  Support 130cm en bois et métal pour bois de ch...   MASSIVUM  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO \n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnhWzCrWyAcD"
   },
   "source": [
    "Afficher les 10 premières lignes du fichier de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IkgZRlqyAcF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324069</th>\n",
       "      <td>1000001700</td>\n",
       "      <td>1000012947</td>\n",
       "      <td>1000013019</td>\n",
       "      <td>AMADEUS Porte télécommandes  - Porte télécomma...</td>\n",
       "      <td>AMADEUS Porte télécommandes</td>\n",
       "      <td>AMADEUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410698</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000014196</td>\n",
       "      <td>1000014197</td>\n",
       "      <td>En verve</td>\n",
       "      <td>Marcel Proust en verve</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659700</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000015308</td>\n",
       "      <td>1000015309</td>\n",
       "      <td>The Canterbury Tales - Chaucer, Geoffrey - The...</td>\n",
       "      <td>The Canterbury Tales - Chaucer, Geoffrey</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283218</th>\n",
       "      <td>1000003924</td>\n",
       "      <td>1000011422</td>\n",
       "      <td>1000011427</td>\n",
       "      <td>HP OfficeJet 6000 special Edition - Original H...</td>\n",
       "      <td>HP OfficeJet 6000 special Edition - Original HP…</td>\n",
       "      <td>HP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354908</th>\n",
       "      <td>1000010220</td>\n",
       "      <td>1000010327</td>\n",
       "      <td>1000010328</td>\n",
       "      <td>Cache Oreilles enfant \"Coloriage\" noir - Goute...</td>\n",
       "      <td>Cache Oreilles enfant \"Coloriage\" noir</td>\n",
       "      <td>LES TRESORS DE LILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812705</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>Coque souple Grise pour SAMSUNG GALAXY TREND m...</td>\n",
       "      <td>Coque souple Grise pour SAMSUNG GALAXY TREND mo…</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534888</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>COQUE CIGALE-2 POUR SAMSUNG GALAXY NOTE 2 COQ0...</td>\n",
       "      <td>COQUE CIGALE-2 POUR SAMSUNG GALAXY NOTE 2 COQ00…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932863</th>\n",
       "      <td>1000002514</td>\n",
       "      <td>1000002515</td>\n",
       "      <td>1000002518</td>\n",
       "      <td>Console en bois Saul - Console en bois Saul. C...</td>\n",
       "      <td>Console en bois Saul</td>\n",
       "      <td>J. LINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898460</th>\n",
       "      <td>1000005258</td>\n",
       "      <td>1000005707</td>\n",
       "      <td>1000005918</td>\n",
       "      <td>Kit Gros Freins K-Sport 6 Pistons VOLKSWAGEN C...</td>\n",
       "      <td>Kit Gros Freins K-Sport 6 Pistons VOLKSWAGEN CO…</td>\n",
       "      <td>AUDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71410</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010667</td>\n",
       "      <td>Housse Portfolio Moxie Venezia Rouge - Nokia L...</td>\n",
       "      <td>Housse Portfolio Moxie Venezia Rouge - Nokia Lu…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Categorie1  Categorie2  Categorie3  \\\n",
       "324069  1000001700  1000012947  1000013019   \n",
       "410698  1000014006  1000014196  1000014197   \n",
       "659700  1000014006  1000015308  1000015309   \n",
       "283218  1000003924  1000011422  1000011427   \n",
       "354908  1000010220  1000010327  1000010328   \n",
       "812705  1000010560  1000010623  1000010653   \n",
       "534888  1000010560  1000010623  1000010653   \n",
       "932863  1000002514  1000002515  1000002518   \n",
       "898460  1000005258  1000005707  1000005918   \n",
       "71410   1000010560  1000010623  1000010667   \n",
       "\n",
       "                                              Description  \\\n",
       "324069  AMADEUS Porte télécommandes  - Porte télécomma...   \n",
       "410698                                           En verve   \n",
       "659700  The Canterbury Tales - Chaucer, Geoffrey - The...   \n",
       "283218  HP OfficeJet 6000 special Edition - Original H...   \n",
       "354908  Cache Oreilles enfant \"Coloriage\" noir - Goute...   \n",
       "812705  Coque souple Grise pour SAMSUNG GALAXY TREND m...   \n",
       "534888  COQUE CIGALE-2 POUR SAMSUNG GALAXY NOTE 2 COQ0...   \n",
       "932863  Console en bois Saul - Console en bois Saul. C...   \n",
       "898460  Kit Gros Freins K-Sport 6 Pistons VOLKSWAGEN C...   \n",
       "71410   Housse Portfolio Moxie Venezia Rouge - Nokia L...   \n",
       "\n",
       "                                                 Libelle               Marque  \n",
       "324069                      AMADEUS Porte télécommandes               AMADEUS  \n",
       "410698                            Marcel Proust en verve               AUCUNE  \n",
       "659700          The Canterbury Tales - Chaucer, Geoffrey               AUCUNE  \n",
       "283218  HP OfficeJet 6000 special Edition - Original HP…                   HP  \n",
       "354908            Cache Oreilles enfant \"Coloriage\" noir  LES TRESORS DE LILY  \n",
       "812705  Coque souple Grise pour SAMSUNG GALAXY TREND mo…              MUZZANO  \n",
       "534888  COQUE CIGALE-2 POUR SAMSUNG GALAXY NOTE 2 COQ00…               AUCUNE  \n",
       "932863                              Console en bois Saul              J. LINE  \n",
       "898460  Kit Gros Freins K-Sport 6 Pistons VOLKSWAGEN CO…                 AUDI  \n",
       "71410   Housse Portfolio Moxie Venezia Rouge - Nokia Lu…               AUCUNE  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8LVXOsNyAcM"
   },
   "source": [
    "## 3. Nettoyage des données\n",
    "\n",
    "Nous devons traiter des données textuelles et il nous faudra donc construire une représentation numérique de ces données. Pour cela, il est d'abord nécessaire de nettoyer ces données. \n",
    "\n",
    "Dans cette étude de cas, nous représenterons les données à partir de la liste des mots les constituant et il faudra donc :\n",
    " + Construire un dictionnaire de mots. Ce dictionnaire de mot sera l'espace de représentation de vos données. Pour cela, il  vous faut : \n",
    "  + Découper le texte en mots.\n",
    "   + Nettoyer le texte, le simplifier : suppression des ponctuations, des termes numériques, des caractéres mal codés, passage de tous les mots en minuscules.\n",
    "   + Supprimer les mots non porteurs de sens (ou stop words) à l'aide de la liste `lucene_stopwords.txt`.\n",
    "   + Lemmatiser ou Raciniser (transformer un mot en sa forme canonique) afin de reduire la taille du dictionnaire et donc l'espace de représentation des données.\n",
    "\n",
    "Vous vous appuirez pour cela sur la bibliothèque `nltk`. Si vous ne l'avez jamais utilisé, prenez le temps de regarder ce petit tutoriel [ici](https://code.tutsplus.com/fr/tutorials/introducing-the-natural-language-toolkit-nltk--cms-28620)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87jxRRn7yAcP"
   },
   "source": [
    "### Création de la liste de stop words\n",
    "Combiner la liste de stop-words de nltk (français) avec celle fournie dans le fichier `lucene_stopwords.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjT4GOlWyAcQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marchand'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO : listes de mots à supprimer dans la description des produits\n",
    "from nltk.corpus import stopwords\n",
    "#print(set(stopwords.words('French')))\n",
    "## Depuis NLTK\n",
    "  # TO COMPLETE\n",
    "stopwords_1 = list(set(stopwords.words('French')))\n",
    "## Depuis le fichier fourni\n",
    "  # TO COMPLETE\n",
    "with open(\"./Data/lucene_stopwords.txt\", \"r\" ) as txt:\n",
    "    stopwords_2 = txt.read().split(',')\n",
    "\n",
    "    \n",
    "## Union des deux fichiers de stopwords \n",
    "  # TO COMPLETE\n",
    "stopwords = set(stopwords_1 + stopwords_2)\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "## Fonction de stemming permettant la racinisation pour la language française (ntlk : SnowballStemmmer)\n",
    "  # TO COMPLETE\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def stemmer(word, language):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "language = 'french'\n",
    "word = 'Marchand'\n",
    "stemmer(word, language)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kNt5BmjyAcS"
   },
   "source": [
    "### Fonction de nettoyage\n",
    "\n",
    "Ecrire une fonction de nettoyage de texte qui prend en entrée un texte et applique les étapes suivantes:\n",
    " + Nettoyage des données HTML avec la bibliothèque [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#)\n",
    " + Passage en minuscule\n",
    " + Encodage uniforme\n",
    " + Suppression des caractères non alpha numériques (ponctuations) : pensez aux expressions régulières (module `re` - voire la documentation ici)\n",
    " + Suppression des stop-words\n",
    " + Racinisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Twm-ig8ZyAcT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata \n",
    "import string \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "\n",
    "def clean_txt(txt):\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf_8').get_text()\n",
    "    ###  TO DO : lower case\n",
    "    txt = txt.lower()\n",
    "\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    \n",
    "    #print(txt)\n",
    "    txt = unicodedata.normalize('NFKD',txt).encode('ascii', 'ignore')\n",
    "    txt = txt.decode('ascii')\n",
    "    #print(txt)\n",
    "    \n",
    "\n",
    "    ### remove non alphanumeric char\n",
    "    remove = string.punctuation + '.'+ '-' + '('+ ')'\n",
    "    table = str.maketrans({key: None for key in remove})\n",
    "    txt = txt.translate(table)\n",
    "    #re.sub('[\\W_]+', '', txt)    \n",
    "    \n",
    "    ### remove french stop words\n",
    "    parsed_txt = word_tokenize(txt)\n",
    "    parsed_txt = [x for x in parsed_txt if x not in stopwords]\n",
    "    \n",
    "    ### french stemming\n",
    "    stem = [stemmer(x, 'french') for x in parsed_txt]\n",
    "    return ' '.join(stem)\n",
    "\n",
    "    ### tokens = stemmer.stemWords(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcthanvancon/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chamad francois sagan chamad francois sagan edit ren julliard 1971 broch voir present'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt(df.iloc[0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mvTGiDxyAcX"
   },
   "source": [
    "### Nettoyage de la marque\n",
    "\n",
    "Il s'agit ici de nettoyer le champ marque en ne gardant que les caractères alpha-numériques (module `re`, documentation [ici](http://www.xavierdupre.fr/app/teachpyx/helpsphinx/c_regex/regex.html) ou [là](https://openclassrooms.com/fr/courses/235344-apprenez-a-programmer-en-python/233857-les-expressions-regulieres) et en passant en minuscules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wov6g0ryAcY"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "def clean_marque(txt):\n",
    "    remove = string.punctuation + '.'+ '-' + '('+ ')'\n",
    "    table = str.maketrans({key: None for key in remove}) \n",
    "    try:\n",
    "        return txt.translate(table)\n",
    "    except Exception:\n",
    "        pass\n",
    "# TO DO : test sur une ligne     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wd39kltyAca"
   },
   "source": [
    "### Nettoyage de l'ensemble des données\n",
    "\n",
    "Il s'agit ici d'appliquer le nettoyage à l'ensemble des données présentes dans la dataframe. Ecrire une fonction `clean_dataframe` qui permet d'appliquer le nettoyage sur un dataframe. On mesurera ici aussi le temps mis par l'environnement à l'aide du module `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHURS_vEyAcb"
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "\n",
    "def clean_dataframe(input_data, columns):\n",
    "    # TO COMPLETE\n",
    "    start_time = time.time()\n",
    "    df = input_data.copy(deep = True)\n",
    "    for c in columns:\n",
    "        df.loc[:, c] = df.loc[:, c].apply(clean_txt)\n",
    "    stop_time = time.time()\n",
    "    print('elapsed time', stop_time - start_time)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3W353F8UyAce"
   },
   "source": [
    "Appliquer la fonction `clean_dataframe` sur votre ensemble réduit de données de validation et d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4GYyG_IyAcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcthanvancon/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 54.35039687156677\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>coqu soupl noir sony xperi z motif drapeau cam...</td>\n",
       "      <td>Coque souple Noire pour SONY XPERIA Z motif Dra…</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>1000010096</td>\n",
       "      <td>1000010097</td>\n",
       "      <td>1000010136</td>\n",
       "      <td>sikh khand colli coeur or 18 carat plaqu colli...</td>\n",
       "      <td>Sikh Khanda Collier Coeur - or 18 carats plaqu...</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000015308</td>\n",
       "      <td>1000015309</td>\n",
       "      <td>nicol barrier martin glomeron edit harmattan</td>\n",
       "      <td>Attention travail ! recueil de poèmes contempo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19499</th>\n",
       "      <td>1000008694</td>\n",
       "      <td>1000008920</td>\n",
       "      <td>1000008939</td>\n",
       "      <td>tamii 56512 radio command camion rou aluminium...</td>\n",
       "      <td>TAMIYA - 56512 - RADIO COMMANDE - CAMION - ROUE…</td>\n",
       "      <td>TAMIYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010641</td>\n",
       "      <td>cabl synchronis micro usb longueur cabl 20cm</td>\n",
       "      <td>Cable sync micro USB pour Huawei Ascend Y600</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Categorie1  Categorie2  Categorie3  \\\n",
       "17812  1000010560  1000010623  1000010653   \n",
       "9987   1000010096  1000010097  1000010136   \n",
       "3421   1000014006  1000015308  1000015309   \n",
       "19499  1000008694  1000008920  1000008939   \n",
       "8186   1000010560  1000010623  1000010641   \n",
       "\n",
       "                                             Description  \\\n",
       "17812  coqu soupl noir sony xperi z motif drapeau cam...   \n",
       "9987   sikh khand colli coeur or 18 carat plaqu colli...   \n",
       "3421        nicol barrier martin glomeron edit harmattan   \n",
       "19499  tamii 56512 radio command camion rou aluminium...   \n",
       "8186        cabl synchronis micro usb longueur cabl 20cm   \n",
       "\n",
       "                                                 Libelle   Marque  \n",
       "17812   Coque souple Noire pour SONY XPERIA Z motif Dra…  MUZZANO  \n",
       "9987   Sikh Khanda Collier Coeur - or 18 carats plaqu...   AUCUNE  \n",
       "3421   Attention travail ! recueil de poèmes contempo...      NaN  \n",
       "19499   TAMIYA - 56512 - RADIO COMMANDE - CAMION - ROUE…   TAMIYA  \n",
       "8186        Cable sync micro USB pour Huawei Ascend Y600   AUCUNE  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO\n",
    "df_train_lm = clean_dataframe(df_train_lm, ['Description'])\n",
    "df_train_lm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 12.712611198425293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>1000003924</td>\n",
       "      <td>1000003930</td>\n",
       "      <td>1000004079</td>\n",
       "      <td>aliment electr bloc secteur lg f1 aliment elec...</td>\n",
       "      <td>Alimentation électrique / Bloc secteur pour LG F1</td>\n",
       "      <td>POWERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1000014006</td>\n",
       "      <td>1000014196</td>\n",
       "      <td>1000014202</td>\n",
       "      <td>folio</td>\n",
       "      <td>L'Erreur</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18571</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>coqu stuff4 coqu htc one1 m7 galaxy vagu envel...</td>\n",
       "      <td>Coque de Stuff4 / Coque pour HTC One/1 M7 / Ga...</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>193</td>\n",
       "      <td>1449</td>\n",
       "      <td>194</td>\n",
       "      <td>sufro porqu quiero studio cdcec stereo cd trac...</td>\n",
       "      <td>Sufro Porque Te Quiero</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>1000010560</td>\n",
       "      <td>1000010623</td>\n",
       "      <td>1000010653</td>\n",
       "      <td>iphon 5 5 coqu houss protect cuir pu rayur mul...</td>\n",
       "      <td>Iphone 5 - 5S : Coque Housse de Protection Cuir…</td>\n",
       "      <td>AUCUNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Categorie1  Categorie2  Categorie3  \\\n",
       "4153   1000003924  1000003930  1000004079   \n",
       "636    1000014006  1000014196  1000014202   \n",
       "18571  1000010560  1000010623  1000010653   \n",
       "17993         193        1449         194   \n",
       "3303   1000010560  1000010623  1000010653   \n",
       "\n",
       "                                             Description  \\\n",
       "4153   aliment electr bloc secteur lg f1 aliment elec...   \n",
       "636                                                folio   \n",
       "18571  coqu stuff4 coqu htc one1 m7 galaxy vagu envel...   \n",
       "17993  sufro porqu quiero studio cdcec stereo cd trac...   \n",
       "3303   iphon 5 5 coqu houss protect cuir pu rayur mul...   \n",
       "\n",
       "                                                 Libelle  Marque  \n",
       "4153   Alimentation électrique / Bloc secteur pour LG F1  POWERY  \n",
       "636                                             L'Erreur  AUCUNE  \n",
       "18571  Coque de Stuff4 / Coque pour HTC One/1 M7 / Ga...  AUCUNE  \n",
       "17993                            Sufro Porque Te Quiero   AUCUNE  \n",
       "3303    Iphone 5 - 5S : Coque Housse de Protection Cuir…  AUCUNE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_lm = clean_dataframe(df_test_lm, ['Description'])\n",
    "df_test_lm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WocUcNbsyAcl"
   },
   "source": [
    "Appliquer la fonction `clean_dataframe` sur votre ensemble entier de données de validation et d'apprentissage. Attention cette opération peut être un peu longue. Assurez-vous de ne pas utiliser de fonction `print` de manière abusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_nJP29RyAcm"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "df_test = clean_dataframe(df_test, ['Description'])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = clean_dataframe(df_train, ['Description'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReL-lMfhyAcp"
   },
   "source": [
    "## 3.  Représentation des données.\n",
    "\n",
    "Pour représenter nos données (i.e. la description textuelle des produits), pluieurs principes seront utilisés et comparés :\n",
    "\n",
    " + L'approche de représentation d'un document textuel par un sac de mots de type `one_hot_encoding` avec scikit-learn comme expliqué [ici](scikit-learn)\n",
    " + L'approche de représentation d'un document textuel par un sac de mots et une pondération [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) vue dans les premiers cours. De nombreux modules sont disponibles dans scikit-learn, notamment [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation) pour son implémentation.\n",
    " + Une approche de hachage qui est une des techniques utilisées pour le traitement des données massives. Elle consiste à réduire fortement le volume de calculs à faire sur les données en réduisant l'ordre de complexité des calculs à faire par l'exploitation des caractéristiques de similarité des données. Ici aussi, vous pouvez tirer partie des modules existants dans scikit-learn décrits [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick).\n",
    " + Une représentation de type word2vec avec la bibliothèque [gensim](https://radimrehurek.com/gensim/).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tMYmOW-yAcq"
   },
   "source": [
    "#### Sacs de mots (One-Hot-Encoding)\n",
    "\n",
    "Ecrire la fonction ` data_one_hot_encoding_representation` qui construit la représentation `one-hot-encoding` de votre jeu de données de description et qui retourne cette représentation, le vocabulaire obtenu associé ainsi que le vectorizer et la fonction `apply_data_one_hot_encoding_representation` qui utilise le vectorizer pour la représentation du jeu de données de validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uM5vgEPUyAcq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TO DO\n",
    "\n",
    "def data_one_hot_encoding_representation(data_train):\n",
    "    vectorizer = CountVectorizer()\n",
    "    representation = vectorizer.fit_transform(data_train.loc[:,'Description'].tolist())\n",
    "    return vectorizer, vectorizer.get_feature_names(), representation\n",
    "    \n",
    "# TO COMPLETE\n",
    "def apply_data_one_hot_encoding_representation(data_valid,vec):\n",
    "    representation = vec.fit_transform(data_valid.loc[:,'Description'].tolist())\n",
    "    return representation\n",
    "# TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bS8QZ3NsyAcs"
   },
   "source": [
    "Appliquer cette fonction à votre jeu de données d'apprentissage et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGe5q5vzyAct"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "vec, vocab, rpz_train = data_one_hot_encoding_representation(df_train)\n",
    "\n",
    "rpz_test = apply_data_one_hot_encoding_representation(df_test, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpz_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEj7MWGsyAcv"
   },
   "source": [
    "#### Sacs de mots - TF IDF\n",
    "\n",
    "Ecrire la fonction ` data_tf_idf_encoding_representation` qui construit la représentation `TF-IDF` d'un jeu de données et qui retourne cette représentation ainsi que le vocabulaire obtenu associé et le vectorizer et une fonction `apply_data_tf_idf_encoding_representation` qui permet d'appliquer le vectorizer sur le jeu de données de validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOCX3oZCyAcw"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def data_tf_idf_encoding_representation(data_train):\n",
    "# TO COMPLETE\n",
    "\n",
    "def apply_data_tf_idf_encoding_representation(data_valid,vec):\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIuw3g7UyAcy"
   },
   "source": [
    "Appliquer cette fonction à votre jeu de données d'apprentissage et de validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxJf0uAWyAcy"
   },
   "outputs": [],
   "source": [
    "# TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yy5i6yN_yAc0"
   },
   "source": [
    "#### Hachage\n",
    "\n",
    "Ecrire la fonction `data_hash_encoding_representation` qui construit la représentation par hachage de votre jeu de données de description et qui retourne cette représentation et le vectorizer et une fonction `apply_data_hash_encoding_representation` qui permet d'appliquer le vectorizer sur le jeu de données de validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivBkoPYvyAc1"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import collections\n",
    "\n",
    "def data_hash_encoding_representation(data_train,nb_hash):\n",
    "# TO COMPLETE\n",
    "\n",
    "\n",
    "def apply_data_hash_encoding_representation(data_valid,vec):\n",
    "# TO COMPLETE   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCHdecoOyAc3"
   },
   "source": [
    "Appliquer cette fonction à votre jeu de données d'apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8ObmFT0yAc4"
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6f0X-OIzyAc8"
   },
   "source": [
    "#### (Optionel ) Word2Vec\n",
    "\n",
    "Pour représenter les données avec une représentation distribuée, nous pouvons utiliser la bibliothèque `gensim`. La documentation est [ici](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgKkvdb1yAc8"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "\n",
    "import gensim\n",
    "\n",
    "def data_word2vec_representation(data_train):\n",
    "# TO COMPLETE\n",
    "\n",
    "\n",
    "def apply_word2vec_representation(data_valid):\n",
    "# TO COMPLETE   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aP3a-DzhyAc-"
   },
   "source": [
    "### Construction de la représentation\n",
    "\n",
    "Ecrire deux fonctions `vectorizer_train` and `apply_vectorizer` permettant de générer automatiquement différents dataframe d'apprentissage et de validation vectorisés avec les différentes approches précédentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rp0E4LEEyAc_"
   },
   "outputs": [],
   "source": [
    "def vectorizer_train(df, columns=['Description'], nb_hash=None, nb_gram = 1, vectorizer = \"tfidf\" , binary = False):\n",
    "    # TO COMPLETE\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, feathash, columns =['Description']):\n",
    "     # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-sH7B_GyAdC"
   },
   "source": [
    "## 4.  Apprentissage et performance\n",
    "\n",
    "Différentes techniques de classification seront utilisées pour la catégorisation des produits et leur performance seront évaluées et comparées :\n",
    "\n",
    " + Regression Logistique, documentée [ici](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    " + Les arbres de décision avec la méthode CART, documentée [ici](https://scikit-learn.org/stable/modules/tree.html)\n",
    " + Les Random Forests, documentés [ici](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    " \n",
    "Pour chacune des méthodes, il faudra mesurer le temps mis pour apprendre le modèle sur notre jeu de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_p_dQKCyAdE"
   },
   "source": [
    "### Regression Logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdsi30HOyAdF"
   },
   "outputs": [],
   "source": [
    "# Regression Logistique \n",
    "## estimation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOtf3waWyAdH"
   },
   "outputs": [],
   "source": [
    "## erreur en validation\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Iqw4qulyAdJ"
   },
   "source": [
    "### Arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKwEWlowyAdJ"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MemEf8JPyAdL"
   },
   "outputs": [],
   "source": [
    "## erreur en validation\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s49mb1sKyAdN"
   },
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD5zSVgUyAdO"
   },
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OXz9eBSyAdQ"
   },
   "outputs": [],
   "source": [
    "## erreur en validation\n",
    "# TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehr3lGm6yAdT"
   },
   "source": [
    "Vous avez fini ici la mise en place de la chaine de traitements. Pour étudier la performance de cette chaîne dans le contexte de données massives, mesurez l'évolution du temps d'execution du nettoyage, de la représentation et de l'apprentissage en fonction de la taille de l'échantillon d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbT0V6LTyAdU"
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EtudeDeCas_CDiscount_Python_Final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
